{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Exercise_16_05.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 16.05: Guided Exercise**\n",
        "### Spot-Checking models using ML pipelines"
      ],
      "metadata": {
        "id": "zUzUkKZlzZVw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing modules"
      ],
      "metadata": {
        "id": "cR0g8kf8ztpZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHkHVZAlInQn"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Loading data"
      ],
      "metadata": {
        "id": "n-8xxfXKzwf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "filename = 'https://raw.githubusercontent.com/OsamaAkhlaq/DS_Book/main/Chapter%2016/pima-indians-diabetes.csv'\n",
        "# Loading the data using pandas\n",
        "\n",
        "diabData = pd.read_csv(filename,sep=\",\",header = None,na_values = \"?\")\n",
        "diabData.head()"
      ],
      "metadata": {
        "id": "JNpUa4gEI2nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finding number of null values in the data set"
      ],
      "metadata": {
        "id": "YPGG4DX0zzu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "diabData.isnull().sum()"
      ],
      "metadata": {
        "id": "Ov4v5bjZLklM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dropping all the rows with na values"
      ],
      "metadata": {
        "id": "ubxceS8oz1oF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "newdiabdata = diabData.dropna(axis = 0)"
      ],
      "metadata": {
        "id": "CnuBp4dHLnHr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Printing the shape of earlier data set and new data set"
      ],
      "metadata": {
        "id": "zZOa-gFQz3Sf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(diabData.shape)\n",
        "print(newdiabdata.shape)"
      ],
      "metadata": {
        "id": "qIEv91pYOT4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Seperating X and y variables"
      ],
      "metadata": {
        "id": "iC6X2q9Dz6iF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X = newdiabdata.loc[:,0:8]\n",
        "print(X.shape)\n",
        "y = newdiabdata.loc[:,8]\n",
        "print(y.shape)"
      ],
      "metadata": {
        "id": "0OQJq2KoLqVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting the data into train and test sets"
      ],
      "metadata": {
        "id": "Hq736_igz8qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=123)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "sYTQey-PLvKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pipe line for Dummy creation\n"
      ],
      "metadata": {
        "id": "f6xqtDNUPAPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the necessary packages\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "metadata": {
        "id": "eXZ62Js3O-YE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "# Pipeline for transforming categorical variables\n",
        "catTransformer = Pipeline(steps=[('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "# Pipeline for scaling numerical variables\n",
        "numTransformer = Pipeline(steps=[('scaler', StandardScaler())])"
      ],
      "metadata": {
        "id": "GMZ4ihXyPD0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Printing dtypes for X"
      ],
      "metadata": {
        "id": "-wk1G4h80X3B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X.dtypes"
      ],
      "metadata": {
        "id": "DLZzB6gePHoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selecting numerical features"
      ],
      "metadata": {
        "id": "_Q6Ba2qq0bZh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "numFeatures = X.select_dtypes(include=['int64', 'float64']).columns\n",
        "numFeatures"
      ],
      "metadata": {
        "id": "AtYkszFDPigl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Selecting Categorical features"
      ],
      "metadata": {
        "id": "kbN2Gbbl0elN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "catFeatures = X.select_dtypes(include=['object']).columns\n",
        "catFeatures"
      ],
      "metadata": {
        "id": "tjpPlyqyPjTp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the preprocessing engine"
      ],
      "metadata": {
        "id": "OjLPOm0Y0hE3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('numeric', numTransformer, numFeatures),\n",
        "        ('categoric', catTransformer, catFeatures)])"
      ],
      "metadata": {
        "id": "PJUVTmLrPn7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming the Training data\n",
        "Xtran_train = pd.DataFrame(preprocessor.fit_transform(X_train))\n",
        "print(Xtran_train.shape)\n",
        "Xtran_train.head()"
      ],
      "metadata": {
        "id": "oRGcOjkIPqCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transforming Test data\n",
        "Xtran_test = pd.DataFrame(preprocessor.transform(X_test))\n",
        "print(Xtran_test.shape)\n",
        "Xtran_test.head()"
      ],
      "metadata": {
        "id": "-tGkJ7H3Pssg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimensionality reduction after processing with Pipeline"
      ],
      "metadata": {
        "id": "WODS6H96Q7hO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing PCA library\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier"
      ],
      "metadata": {
        "id": "c-38CdKhQ5XB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating an estimator with both preprocessor and dimensionality reduction"
      ],
      "metadata": {
        "id": "7zbd-gVW0m2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "estimator = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                      ('dimred', PCA(5)),\n",
        "                      ('clf',LogisticRegression(random_state=123))])"
      ],
      "metadata": {
        "id": "s4MPjWKSQ9_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a list of the classifiers"
      ],
      "metadata": {
        "id": "N-5oQSRX0pyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "classifiers = [\n",
        "    KNeighborsClassifier(5),     \n",
        "    RandomForestClassifier(random_state=123),\n",
        "    AdaBoostClassifier(random_state=123),\n",
        "    LogisticRegression(random_state=123)\n",
        "    ]"
      ],
      "metadata": {
        "id": "PfBuiD4Tqedl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for classifier in classifiers:\n",
        "    estimator = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                      ('dimred', PCA(5)),\n",
        "                           ('classifier',classifier)])\n",
        "    estimator.fit(X_train, y_train)   \n",
        "    print(classifier)\n",
        "    print(\"model score: %.2f\" % estimator.score(X_test, y_test))"
      ],
      "metadata": {
        "id": "XC696shuqgN7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}